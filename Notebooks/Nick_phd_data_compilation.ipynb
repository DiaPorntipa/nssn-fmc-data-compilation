{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCwWdERUkAxT"
   },
   "source": [
    "# Start\n",
    "\n",
    "This script cleans field data from Nick's PhD and enriches it with slope, aspect, relief, and vegetation cover values.\n",
    "\n",
    "## ðŸ“„ **What this script does**\n",
    "1. **Cleans** the field data by selecting only observations where `Height` equals 1, and `Datetime` is after the installation date/time and before the removal date/time.\n",
    "2. Removes all columns except `SiteID`, `X`, `Y`, `Datetime`, `C`, and `RH`. The `C` column is renamed to `Temperature`.\n",
    "3. **Corrects relative humidity** (`RH`) using equations from the supplementary material of this publication: [https://doi.org/10.1016/j.agrformet.2013.03.008](https://doi.org/10.1016/j.agrformet.2013.03.008).\n",
    "4. **Calculates VPD** (vapor pressure deficit) from the corrected temperature and RH.\n",
    "5. Generates `site_data_summary.csv`, which contains the list of each site (with X and Y coordinates) and the dates for which data is available.\n",
    "6. For each site in `sites_df`, **fills in slope, aspect, and relief** using precomputed `.tif` files from `topography_calculation.ipynb`. These topographic values are then used to enrich the full field dataset.\n",
    "7. The vegetation cover data is derived from the `veg_cover` field of the DEA Fractional Cover product, retrieved via the ARE NCI. For each observation, **the `veg_cover` value is taken from the `veg_cover` data point closest in time and location.**\n",
    "8. The resulting observations, with their slope, aspect, relief, and vegetation cover, are saved as `in-situ_topography.csv`.\n",
    "\n",
    "## âš ï¸ **Important notes**\n",
    "* **Before running the script**, set all variables in the first cell, and delete the second cell if not using a Google Colab environment.  \n",
    "  *(The script was developed for use in Google Colab and has not been tested outside of it.)*\n",
    "* The `DEA_Fractional_cover_veg_cover.nc` file, which contains vegetation cover data from the DEA Fractional Cover product, has an incorrect `crs` attribute. The correct coordinate reference system is EPSG:32754.\n",
    "* For site observations that the corresponding vegetation cover data in DEA Fractional Cover product are `NaN`s, **the nearest valid vegetation cover values in time are used.**\n",
    "\n",
    "# TODO (MEDIUM): Edit veg_cover related content\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EVlEv6AIXRc"
   },
   "outputs": [],
   "source": [
    "# Set variables\n",
    "data_file_name = \"T_RH_2020-11-10(in).csv\"\n",
    "data_url = \"https://anu365.sharepoint.com/:x:/r/sites/ANU-OptusBushfireResearchCoE/Shared%20Documents/Projects/2024_5%20NSSN%20FMC%20monitoring/T_RH_2020-11-10.csv?d=w7e97fe709fc24f839be6d929d48bde0e&csf=1&web=1&e=3XhSoD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_X5h350Hduv",
    "outputId": "2b26f7d6-a07e-4569-a60c-dab2a299ba5c"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from Utils.vpd import calculate_vpd\n",
    "from Utils.barra2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPWnv-rha6X9"
   },
   "outputs": [],
   "source": [
    "# Generate output directory\n",
    "\n",
    "import os\n",
    "output_dir = os.path.join(\"..\", \"output/csv\")\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWaOdGxBIQ-v"
   },
   "outputs": [],
   "source": [
    "# Download data with a link\n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "def download_file(url, filename):\n",
    "  try:\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "    with open(filename, 'wb') as file:\n",
    "      for chunk in response.iter_content(chunk_size=8192):\n",
    "        file.write(chunk)\n",
    "    print(f\"Data downloaded successfully to {filename}\")\n",
    "  except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading data: {e}\")\n",
    "\n",
    "data_path = os.path.join(\"..\", \"Data\", data_file_name)\n",
    "if not os.path.exists(data_path):\n",
    "    download_file(data_url, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKAPU1gaGgK8"
   },
   "source": [
    "# Read and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "4fyfK07GIWHY",
    "outputId": "72551422-5817-4390-fb6d-8dace32749df"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1hYUWVL1NR7",
    "outputId": "185f4da9-205f-4c24-cd93-5593a45df443"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DTgxWPqGlVz"
   },
   "source": [
    "# Data cleaning and correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "32BJGLd51G8T",
    "outputId": "1503d32f-b172-4212-d221-417732c35519"
   },
   "outputs": [],
   "source": [
    "# Convert all dates into datetime objects\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "format = '%m/%d/%Y %H:%M'\n",
    "\n",
    "df['Instalation time'] = df['Instalation time'].astype(str).str.zfill(4)\n",
    "datetime_str = df['Instalation date'].astype(str) + ' ' + df['Instalation time'].str.slice(0, 2) + ':' + df['Instalation time'].str.slice(2, 4)\n",
    "df['Instalation datetime'] = pd.to_datetime(datetime_str, format=format)\n",
    "\n",
    "df['Removal time'] = df['Removal time'].astype(str).str.zfill(4)\n",
    "datetime_str = df['Removal date'].astype(str) + ' ' + df['Removal time'].str.slice(0, 2) + ':' + df['Removal time'].str.slice(2, 4)\n",
    "df['Removal datetime'] = pd.to_datetime(datetime_str, format=format)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=format)\n",
    "df = df.rename(columns={'Date': 'Datetime'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NXPz_CBkSVhF",
    "outputId": "727eafc2-2ef6-4247-f191-9d3840610088"
   },
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "df = df[df['Height'] != 'zero']  # 120 to 108 sites\n",
    "df = df[(df['Datetime'] > df['Instalation datetime']) & (df['Datetime'] < df['Removal datetime'])]  # 108 to 103 sites\n",
    "df = df[['SiteID', 'X', 'Y', 'Datetime', 'C', 'RH']]\n",
    "df = df.rename(columns={'C': 'Temperature'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xWGNzc7440o",
    "outputId": "7f31e3f7-fa92-4e78-d2c4-d077422a1724"
   },
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "\n",
    "print(\"Latlon bound of dataframe\")\n",
    "df['X'].min(), df['X'].max(), df['Y'].min(), df['Y'].max()\n",
    "\n",
    "print(\"Datetime bound of dataframe\")\n",
    "df['Datetime'].min(), df['Datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "h9PTMShORBVF",
    "outputId": "84f403a4-5cf0-44bf-ec64-61fbc9d699b2"
   },
   "outputs": [],
   "source": [
    "# Handle 'RH' > 100\n",
    "\n",
    "# Find MaxRH for each site and date\n",
    "df['Date'] = df.apply(lambda row: row['Datetime'].date(), axis=1)\n",
    "df['MaxRH'] = df.groupby(['SiteID', 'Date'])['RH'].transform('max')\n",
    "\n",
    "# Find MaxRH95 for each site\n",
    "df['MaxRH95'] = df.groupby(['SiteID'])['MaxRH'].transform(lambda x: x.quantile(0.95))\n",
    "\n",
    "# Apply correction the RH values. If RH is greater than MaxRH95, RH = 100, else RH = RH*100/MaxRH95\n",
    "df['RH'] = df.apply(lambda row: 100 if (row['RH'] > row['MaxRH95']) | (row['RH'] < 0) else  row['RH'] * 100 / row['MaxRH95'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GR2NMdV-8iI6",
    "outputId": "2a877964-c6f3-46bd-84f8-9fdc5c18065a"
   },
   "outputs": [],
   "source": [
    "# Check the correctness of the calculation\n",
    "# df.groupby(['MaxRH95'])['SiteID'].apply(lambda x: x.unique()).reset_index()\n",
    "\n",
    "df.drop(columns=['MaxRH', 'MaxRH95', 'Date'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QLrYKoO2RFYh",
    "outputId": "12ef572e-c5e5-4557-c5fd-12ec66cd8ece"
   },
   "outputs": [],
   "source": [
    "# Calculate VPD\n",
    "\n",
    "df['VPD'] = df.apply(lambda row: calculate_vpd(row['Temperature'], row['RH']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUVYDrhrTCG6"
   },
   "source": [
    "# Get a list of each site (x and y coordinates) and the dates we have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "PuS2HX03TLRh",
    "outputId": "a1a36a6a-04b4-4e61-d2d5-d9c148a8581d"
   },
   "outputs": [],
   "source": [
    "# Group by SiteID, X, Y and collect unique sorted dates\n",
    "sites_df = (\n",
    "    df.groupby(['SiteID', 'X', 'Y'])['Datetime']\n",
    "    .apply(lambda x: ', '.join(sorted(x.dt.strftime('%Y-%m-%d').unique())))\n",
    "    .reset_index()  # Convert the data in Series to DataFrame\n",
    "    .rename(columns={'Datetime': 'Dates'})\n",
    ")\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_df = (\n",
    "    df.groupby(['SiteID', 'X', 'Y'])['Datetime']\n",
    "    .apply(lambda x: min(x).strftime('%Y-%m-%d'))\n",
    "    .reset_index()  # Convert the data in Series to DataFrame\n",
    "    .rename(columns={'Datetime': 'start_date'})\n",
    ")\n",
    "start_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_df = (\n",
    "    df.groupby(['SiteID', 'X', 'Y'])['Datetime']\n",
    "    .apply(lambda x: min(x).strftime('%Y-%m-%d'))\n",
    "    .reset_index()  # Convert the data in Series to DataFrame\n",
    "    .rename(columns={'Datetime': 'end_date'})\n",
    ")\n",
    "end_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = sites_df.merge(start_date_df)\n",
    "sites_df = sites_df.merge(end_date_df)\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y-0bl2DXM5i"
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "sites_df.to_csv(os.path.join(output_dir, 'site_data_summary_phd.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_urPim__GtvE"
   },
   "source": [
    "# Add biophysical properties into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GmsKZWfE6EJh",
    "outputId": "15af84db-902e-4fdf-8410-7e28aa858315"
   },
   "outputs": [],
   "source": [
    "!pip install rasterio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRxLZ1NxSaEp"
   },
   "source": [
    "## Fill in slope, aspect, and relief of each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGKJ6x3ZJKTy"
   },
   "outputs": [],
   "source": [
    "# Add slope, aspect, and relief data\n",
    "# TODO (LOW) : The biophysical data are retrieved several time for each sites. Make it more efficient.\n",
    "\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import rowcol\n",
    "from rasterio.warp import transform\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def get_value_from_latlon(data, crs, trans, x, y):\n",
    "    # Find corresponding row and column with latlon\n",
    "    x_proj, y_proj = transform('EPSG:4326', crs, [x], [y])\n",
    "    row, col = rowcol(trans, x_proj, y_proj)\n",
    "\n",
    "    try:\n",
    "        return data[row, col][0]\n",
    "    except:\n",
    "        print(f\"There is a row with lon {x}, lat {y} outside tif files' area. The value is set to np.nan\")\n",
    "        return np.nan\n",
    "\n",
    "def add_data_to_df(property, df):\n",
    "    # TODO (LOW): There is no more need to merge all tif files.\n",
    "    # Find all related tif files\n",
    "    files = glob.glob(os.path.join(\"..\", 'output', f\"*{property}.tif\"))\n",
    "    print(f\"There are {len(files)} {property} files: {files}\")\n",
    "\n",
    "    # Merge all tif files\n",
    "    src_files_to_mosaic = [rasterio.open(f) for f in files]\n",
    "    mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "    data = mosaic[0]\n",
    "\n",
    "    # Add data to df\n",
    "    crs = src_files_to_mosaic[0].crs\n",
    "    # # TODO (LOW): I was trying to resolving rows and cols calculation redundancy\n",
    "    # if \"temp_row\" not in df.columns or \"temp_col\" not in df.columns:\n",
    "    #     x_proj, y_proj = transform('EPSG:4326', crs, df['X'], df['Y'])\n",
    "    #     df['temp_row'], df['temp_col'] = rowcol(out_trans, x_proj, y_proj)\n",
    "    # df[property] = df.apply(lambda row: data[row['temp_row'], row['temp_col']], axis=1)\n",
    "    df[property] = df.apply(lambda row: get_value_from_latlon(data, crs, out_trans, row['X'], row['Y']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "SgWxjt8aEnLW",
    "outputId": "d590def5-2dc1-4ff0-a9d5-6305af4dc919"
   },
   "outputs": [],
   "source": [
    "sites_df = add_data_to_df('slope', sites_df)\n",
    "sites_df = add_data_to_df('aspect', sites_df)\n",
    "sites_df = add_data_to_df('relief', sites_df)\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "3cqoUJKLQz4t",
    "outputId": "8a2e872e-e95b-4dfb-d5b1-1ca7cfd890a1"
   },
   "outputs": [],
   "source": [
    "# Investigate missing values\n",
    "# TODO (MEDIUM): There are 5 sites in total with NaNs. Explore whether we can leave it as is.\n",
    "# NOTE: Site id 251 has row: [23133], col: [1081].\n",
    "sites_with_nans_df = sites_df[(sites_df['slope'].isna()) | (sites_df['aspect'].isna()) | (sites_df['relief'].isna())]\n",
    "sites_with_nans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ss3NXUTHXYlx",
    "outputId": "34fa6637-aa97-44da-ca5b-a2f765314e6a"
   },
   "outputs": [],
   "source": [
    "# Add the properties to df\n",
    "df = df.merge(sites_df[[\"SiteID\", \"slope\", \"aspect\", \"relief\"]], on=['SiteID'], how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTkxvLx6_M1W",
    "outputId": "ed6e3307-ca15-435f-d47d-f3adddf43253"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaB8EO9J9yru",
    "outputId": "6490c6ea-2480-425a-ce38-7693cb2bcffa"
   },
   "outputs": [],
   "source": [
    "df = df[df['SiteID'] != 251]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzQDsffHV81o"
   },
   "source": [
    "## Fill in vegetation cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "Fv2wrbHt9CrZ",
    "outputId": "621cc0d3-2f67-44bb-8ecf-93293d114d78"
   },
   "outputs": [],
   "source": [
    "# Load veg cover NetCFD\n",
    "# TODO(LOW): Correct the incorrectly saved 'crs' attribute of veg cover NetCFD from ARE NCI\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "veg_cover_data_path = os.path.join(\"..\", \"Data\", \"Vegetation_cover\", \"veg_cover_phd.nc\")\n",
    "veg_ds = xr.open_dataset(veg_cover_data_path)\n",
    "print(\"The actual crs of the veg_ds is EPSG:32754. The attribute was incorrectly saved when downloading the data.\")\n",
    "veg_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "2G_-HSNtQlMy",
    "outputId": "127941d9-bf29-4ddb-db47-534bf7da7d06"
   },
   "outputs": [],
   "source": [
    "# Visualise veg_cover of the first time step\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "veg_ds[\"veg_cover\"].isel(time=0).plot()\n",
    "plt.title(\"veg_cover layer (e.g. time=0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "_YPA9EF5AdQd",
    "outputId": "f6e950b6-aeda-4f81-ca8d-a663b2638f90"
   },
   "outputs": [],
   "source": [
    "# Find coordinate of each observations in EPSG:32754\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32754\", always_xy=True)\n",
    "df['dea_x'], df['dea_y'] = transformer.transform(df['X'].values, df['Y'].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNSUuwdkIBFQ"
   },
   "outputs": [],
   "source": [
    "# Fill in veg cover in df row by row\n",
    "\n",
    "def get_veg_cover(row):\n",
    "    x = row['dea_x']\n",
    "    y = row['dea_y']\n",
    "    t = row['Datetime']\n",
    "\n",
    "    try:\n",
    "        # Select data at the given location across all times\n",
    "        point_series = veg_ds.sel(x=x, y=y, method='nearest')['veg_cover']\n",
    "\n",
    "        # Drop NaNs\n",
    "        valid_series = point_series.dropna(dim='time')\n",
    "\n",
    "        if valid_series.sizes['time'] == 0:\n",
    "            return np.nan\n",
    "\n",
    "        # Find the nearest time, within 30 days, with non-NaN veg_cover\n",
    "        time_deltas = np.abs(valid_series['time'] - np.datetime64(t))\n",
    "        if time_deltas.min() > np.timedelta64(30, 'D'):\n",
    "            print(\"min time_deltas: \", time_deltas.min() / np.timedelta64(1, 'D'))\n",
    "            return np.nan\n",
    "        nearest_idx = time_deltas.argmin().item()\n",
    "        veg_cover = valid_series.isel(time=nearest_idx).item()\n",
    "\n",
    "        return veg_cover if veg_cover < 100 else 100\n",
    "\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "df['veg_cover'] = df.apply(get_veg_cover, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKdd-fKERS-_"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['dea_x', 'dea_y'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQSVyLZFSHYB"
   },
   "source": [
    "# Save df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw6bn10MNMzU"
   },
   "outputs": [],
   "source": [
    "# save df\n",
    "df.to_csv(os.path.join(output_dir, 'in-situ_topography_phd.csv'), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zioYV7qrLR52"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G98kJmH5OkCP"
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QfS4Kh2FUeW"
   },
   "source": [
    "## Investigation of correctness of codes above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSxCldxP-2By"
   },
   "outputs": [],
   "source": [
    "# Check for accuracy of the veg_cover section\n",
    "\n",
    "# Check for NaN and deal with it\n",
    "df.isna().sum()  # 9661 rows\n",
    "\n",
    "# Check for sites with some rows with NaN veg_cover\n",
    "df[df[\"veg_cover\"].isna()][\"SiteID\"].unique()  # Not all sites\n",
    "\n",
    "# Check for sites with all rows with NaN veg_cover\n",
    "nan_sites = df.groupby(\"SiteID\")[\"veg_cover\"].apply(lambda x: x.isna().all())\n",
    "nan_siteIDs = nan_sites[nan_sites].index.tolist()\n",
    "nan_siteIDs  # 0 site\n",
    "\n",
    "# Inspect different dates\n",
    "df[df['Datetime'].dt.strftime('%Y-%m-%d') == '2018-12-21']  # 120 rows\n",
    "df[df['Datetime'].dt.strftime('%Y-%m-%d') == '2019-01-20'].isna().sum()  # 48 rows\n",
    "df[df['Datetime'].dt.strftime('%Y-%m-%d') == '2019-01-20']  # 120 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpnD_Lgz6R9y"
   },
   "outputs": [],
   "source": [
    "# Investigate whether I need to redownload the veg_cover data from NCI because of its insufficient coverage\n",
    "\n",
    "# Check whether the veg_cover of SiteID 67, 68, 69, 70 on 2018-12-21 are NaN or not\n",
    "df = pd.read_csv(os.path.join(output_dir, 'in-situ_topography_with_NaN_veg_cover.csv'))\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "first_date_df = df[df['SiteID'].isin([67, 68, 69, 70]) & (df['Datetime'].dt.strftime('%Y-%m-%d') == '2018-12-21')]\n",
    "first_date_df['SiteID'].unique()\n",
    "\n",
    "# Check the last date\n",
    "last_date_df = df[df['SiteID'].isin([302, 303, 304, 305]) & (df['Datetime'].dt.strftime('%Y-%m-%d') == '2020-11-06')]\n",
    "last_date_df['SiteID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEW2wR5lBA2Z"
   },
   "outputs": [],
   "source": [
    "# Investigate site 302\n",
    "\n",
    "df = pd.read_csv(os.path.join(output_dir, 'in-situ_topography.csv'))\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "df[df['SiteID'].isin([302]) & (df['Datetime'].dt.strftime('%Y-%m-%d') == '2020-11-06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXobdojy_TGB"
   },
   "outputs": [],
   "source": [
    "# Investigate change in vegetation cover.\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32754\", always_xy=True)\n",
    "dea_x, dea_y = transformer.transform(150.3096, -35.46728)\n",
    "point_series = veg_ds.sel(x=dea_x, y=dea_y, method='nearest')['veg_cover']\n",
    "print(point_series.values)\n",
    "\n",
    "# visualise the point_series\n",
    "point_series.plot()\n",
    "plt.title(\"Vegetation cover of site 302\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GerMk1ZUXbEq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dem_path = os.path.join(\"..\", \"output\", \"batemans_bay_dem_no_neg.tif\")\n",
    "with rasterio.open(dem_path) as src:\n",
    "    dem_data = src.read(1)\n",
    "\n",
    "    lon = sites_with_nans_df['X'].tolist()\n",
    "    lat = sites_with_nans_df['Y'].tolist()\n",
    "    x_proj, y_proj = transform('EPSG:4326', src.crs, lon, lat)\n",
    "    rows, cols = rowcol(src.transform, x_proj, y_proj)\n",
    "\n",
    "plt.imshow(dem_data, cmap='terrain')\n",
    "plt.scatter(cols, rows, color='red', s=10)\n",
    "plt.colorbar(label='DEM (metres)')\n",
    "plt.title('DEM Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sleutg7NFg5m"
   },
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bStvCQ4PJtY8"
   },
   "outputs": [],
   "source": [
    "url = \"https://data.dea.ga.gov.au/derivative/ga_ls_fc_3/2-5-1/090/084/2015/01/02/ga_ls_fc_3_090084_2015-01-02_final_veg_cover.tif\"\n",
    "download_file(url, os.path.join(\"..\", \"data\", \"test.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-jgsDa67A1z"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(mosaic[0], cmap='terrain')\n",
    "plt.colorbar(label='Slope (degrees)')\n",
    "plt.title('Slope Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GShdEyN3SHDs"
   },
   "outputs": [],
   "source": [
    "# TODO (HIGH): Update df with the new code. Below is not usable.\n",
    "# add_data_to_df('slope')\n",
    "# orig_len = len(df)\n",
    "# df = df[(df['slope'] != np.nan) & (df['slope'] != -9999.0)]\n",
    "# print(\"Remove rows with no slope data.\")\n",
    "# print(f\"There are {orig_len - len(df)} rows out of {orig_len} rows with no slope data.\")\n",
    "# add_data_to_df('aspect')\n",
    "# add_data_to_df('relief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ih_dsR9kUPmr"
   },
   "outputs": [],
   "source": [
    "# Visualise relief calculated\n",
    "# TODO (LOW): Correct scaling of the plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relief_path = os.path.join(\"..\", \"output\", \"batemans_bay_relief.tif\")\n",
    "with rasterio.open(relief_path) as src:\n",
    "    relief_data = src.read(1)\n",
    "\n",
    "    lon = sites_with_nans_df['X'].tolist()\n",
    "    lat = sites_with_nans_df['Y'].tolist()\n",
    "    x_proj, y_proj = transform('EPSG:4326', src.crs, lon, lat)\n",
    "    rows, cols = rowcol(src.transform, x_proj, y_proj)\n",
    "\n",
    "plt.imshow(relief_data, cmap='terrain')\n",
    "plt.scatter(cols, rows, color='red', s=10)\n",
    "plt.colorbar(label='Relief (metres)')\n",
    "plt.title('Relief Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3bpnpy2hViH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8YVYex8KvTne1GXcKYa0k",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
