{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eCwWdERUkAxT"
   },
   "source": [
    "# Start\n",
    "\n",
    "This script cleans field data from Nick's PhD and enriches it with slope, aspect, relief, and vegetation cover values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EVlEv6AIXRc"
   },
   "outputs": [],
   "source": [
    "# Set variables\n",
    "working_dir = '../..'  # This repository's root directory\n",
    "data_file_name = \"T_RH_2020-11-10(in).csv\"\n",
    "data_url = \"https://anu365.sharepoint.com/:x:/r/sites/ANU-OptusBushfireResearchCoE/Shared%20Documents/Projects/2024_5%20NSSN%20FMC%20monitoring/T_RH_2020-11-10.csv?d=w7e97fe709fc24f839be6d929d48bde0e&csf=1&web=1&e=3XhSoD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_X5h350Hduv",
    "outputId": "2b26f7d6-a07e-4569-a60c-dab2a299ba5c"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(working_dir)\n",
    "from Utils.barra2 import *\n",
    "from Utils.datetime import *\n",
    "from Utils.rh import handle_exceeding_rh\n",
    "from Utils.vpd import calculate_vpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPWnv-rha6X9"
   },
   "outputs": [],
   "source": [
    "# Generate output directory\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = os.path.join(working_dir, 'output', 'csv')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWaOdGxBIQ-v"
   },
   "outputs": [],
   "source": [
    "# Download data with a link\n",
    "\n",
    "import os\n",
    "\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"Data downloaded successfully to {filename}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading data: {e}\")\n",
    "\n",
    "\n",
    "data_path = os.path.join(working_dir, 'Data', 'phd', data_file_name)\n",
    "if not os.path.exists(data_path):\n",
    "    download_file(data_url, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKAPU1gaGgK8"
   },
   "source": [
    "# Read and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "4fyfK07GIWHY",
    "outputId": "72551422-5817-4390-fb6d-8dace32749df"
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k1hYUWVL1NR7",
    "outputId": "185f4da9-205f-4c24-cd93-5593a45df443"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DTgxWPqGlVz"
   },
   "source": [
    "# Data cleaning and correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "32BJGLd51G8T",
    "outputId": "1503d32f-b172-4212-d221-417732c35519"
   },
   "outputs": [],
   "source": [
    "# Convert all dates into datetime objects\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "format = '%m/%d/%Y %H:%M'\n",
    "\n",
    "df['Instalation time'] = df['Instalation time'].astype(str).str.zfill(4)\n",
    "datetime_str = (\n",
    "    df['Instalation date'].astype(str)\n",
    "    + ' '\n",
    "    + df['Instalation time'].str.slice(0, 2)\n",
    "    + ':'\n",
    "    + df['Instalation time'].str.slice(2, 4)\n",
    ")\n",
    "df['Instalation datetime'] = pd.to_datetime(datetime_str, format=format)\n",
    "\n",
    "df['Removal time'] = df['Removal time'].astype(str).str.zfill(4)\n",
    "datetime_str = (\n",
    "    df['Removal date'].astype(str)\n",
    "    + ' '\n",
    "    + df['Removal time'].str.slice(0, 2)\n",
    "    + ':'\n",
    "    + df['Removal time'].str.slice(2, 4)\n",
    ")\n",
    "df['Removal datetime'] = pd.to_datetime(datetime_str, format=format)\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=format)\n",
    "df = df.rename(columns={'Date': 'Datetime'})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NXPz_CBkSVhF",
    "outputId": "727eafc2-2ef6-4247-f191-9d3840610088"
   },
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "df = df[df['Height'] != 'zero']  # 120 to 108 sites\n",
    "df = df[\n",
    "    (df['Datetime'] > df['Instalation datetime']) & (df['Datetime'] < df['Removal datetime'])\n",
    "]  # 108 to 103 sites\n",
    "df = df[['SiteID', 'X', 'Y', 'Datetime', 'C', 'RH']]\n",
    "df = df.rename(columns={'C': 'Temperature'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4xWGNzc7440o",
    "outputId": "7f31e3f7-fa92-4e78-d2c4-d077422a1724"
   },
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "\n",
    "print(\"Latlon bound of dataframe\")\n",
    "df['X'].min(), df['X'].max(), df['Y'].min(), df['Y'].max()\n",
    "\n",
    "print(\"Datetime bound of dataframe\")\n",
    "df['Datetime'].min(), df['Datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "h9PTMShORBVF",
    "outputId": "84f403a4-5cf0-44bf-ec64-61fbc9d699b2"
   },
   "outputs": [],
   "source": [
    "# Handle 'RH' > 100\n",
    "\n",
    "df = handle_exceeding_rh(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QLrYKoO2RFYh",
    "outputId": "12ef572e-c5e5-4557-c5fd-12ec66cd8ece"
   },
   "outputs": [],
   "source": [
    "# Calculate VPD\n",
    "\n",
    "df['VPD'] = df.apply(lambda row: calculate_vpd(row['Temperature'], row['RH']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUVYDrhrTCG6"
   },
   "source": [
    "# Get a list of each site (x and y coordinates) and the dates we have data for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "PuS2HX03TLRh",
    "outputId": "a1a36a6a-04b4-4e61-d2d5-d9c148a8581d"
   },
   "outputs": [],
   "source": [
    "# Group by SiteID, X, Y and collect unique sorted dates\n",
    "sites_df = (\n",
    "    df.groupby(['SiteID', 'X', 'Y'])['Datetime']\n",
    "    .apply(lambda x: ', '.join(sorted(x.dt.strftime('%Y-%m-%d').unique())))\n",
    "    .reset_index()  # Convert the data in Series to DataFrame\n",
    "    .rename(columns={'Datetime': 'Dates'})\n",
    ")\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_df = (\n",
    "    df.groupby(['SiteID', 'X', 'Y'])['Datetime']\n",
    "    .apply(lambda x: min(x).strftime('%Y-%m-%d'))\n",
    "    .reset_index()  # Convert the data in Series to DataFrame\n",
    "    .rename(columns={'Datetime': 'start_date'})\n",
    ")\n",
    "start_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date_df = (\n",
    "    df.groupby(['SiteID', 'X', 'Y'])['Datetime']\n",
    "    .apply(lambda x: max(x).strftime('%Y-%m-%d'))\n",
    "    .reset_index()  # Convert the data in Series to DataFrame\n",
    "    .rename(columns={'Datetime': 'end_date'})\n",
    ")\n",
    "end_date_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_df = sites_df.merge(start_date_df)\n",
    "sites_df = sites_df.merge(end_date_df)\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y-0bl2DXM5i"
   },
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "sites_df.to_csv(os.path.join(working_dir, 'Data', 'phd', 'site_data_summary_phd.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_urPim__GtvE"
   },
   "source": [
    "# Add biophysical properties into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRxLZ1NxSaEp"
   },
   "source": [
    "## Fill in slope, aspect, and relief of each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGKJ6x3ZJKTy"
   },
   "outputs": [],
   "source": [
    "# Add slope, aspect, and relief data\n",
    "# TODO (LOW) : The biophysical data are retrieved several time for each sites. Make it more efficient.\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.transform import rowcol\n",
    "from rasterio.warp import transform\n",
    "\n",
    "\n",
    "def get_value_from_latlon(data, crs, trans, x, y):\n",
    "    # Find corresponding row and column with latlon\n",
    "    x_proj, y_proj = transform('EPSG:4326', crs, [x], [y])\n",
    "    row, col = rowcol(trans, x_proj, y_proj)\n",
    "\n",
    "    try:\n",
    "        return data[row, col][0]\n",
    "    except:\n",
    "        print(\n",
    "            f\"There is a row with lon {x}, lat {y} outside tif files' area. The value is set to np.nan\"\n",
    "        )\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def add_data_to_df(property, df):\n",
    "    # TODO (LOW): There is no more need to merge all tif files.\n",
    "    # Find all related tif files\n",
    "    files = glob.glob(os.path.join(working_dir, 'output', f'*{property}.tif'))\n",
    "    print(f\"There are {len(files)} {property} files: {files}\")\n",
    "\n",
    "    # Merge all tif files\n",
    "    src_files_to_mosaic = [rasterio.open(f) for f in files]\n",
    "    mosaic, out_trans = merge(src_files_to_mosaic)\n",
    "    data = mosaic[0]\n",
    "\n",
    "    # Add data to df\n",
    "    crs = src_files_to_mosaic[0].crs\n",
    "    # # TODO (LOW): I was trying to resolving rows and cols calculation redundancy\n",
    "    # if \"temp_row\" not in df.columns or \"temp_col\" not in df.columns:\n",
    "    #     x_proj, y_proj = transform('EPSG:4326', crs, df['X'], df['Y'])\n",
    "    #     df['temp_row'], df['temp_col'] = rowcol(out_trans, x_proj, y_proj)\n",
    "    # df[property] = df.apply(lambda row: data[row['temp_row'], row['temp_col']], axis=1)\n",
    "    df[property] = df.apply(\n",
    "        lambda row: get_value_from_latlon(data, crs, out_trans, row['X'], row['Y']), axis=1\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "SgWxjt8aEnLW",
    "outputId": "d590def5-2dc1-4ff0-a9d5-6305af4dc919"
   },
   "outputs": [],
   "source": [
    "sites_df = add_data_to_df('slope', sites_df)\n",
    "sites_df = add_data_to_df('aspect', sites_df)\n",
    "sites_df = add_data_to_df('relief', sites_df)\n",
    "sites_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "3cqoUJKLQz4t",
    "outputId": "8a2e872e-e95b-4dfb-d5b1-1ca7cfd890a1"
   },
   "outputs": [],
   "source": [
    "# Investigate missing values\n",
    "# TODO (MEDIUM): There are 5 sites in total with NaNs. Explore whether we can leave it as is.\n",
    "# NOTE: Site id 251 has row: [23133], col: [1081].\n",
    "sites_with_nans_df = sites_df[\n",
    "    (sites_df['slope'].isna()) | (sites_df['aspect'].isna()) | (sites_df['relief'].isna())\n",
    "]\n",
    "sites_with_nans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "ss3NXUTHXYlx",
    "outputId": "34fa6637-aa97-44da-ca5b-a2f765314e6a"
   },
   "outputs": [],
   "source": [
    "# Add the properties to df\n",
    "df = df.merge(sites_df[[\"SiteID\", \"slope\", \"aspect\", \"relief\"]], on=['SiteID'], how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTkxvLx6_M1W",
    "outputId": "ed6e3307-ca15-435f-d47d-f3adddf43253"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaB8EO9J9yru",
    "outputId": "6490c6ea-2480-425a-ce38-7693cb2bcffa"
   },
   "outputs": [],
   "source": [
    "df = df[df['SiteID'] != 251]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzQDsffHV81o"
   },
   "source": [
    "## Fill in vegetation cover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "Fv2wrbHt9CrZ",
    "outputId": "621cc0d3-2f67-44bb-8ecf-93293d114d78"
   },
   "outputs": [],
   "source": [
    "# Load veg cover NetCFD\n",
    "# TODO(LOW): Correct the incorrectly saved 'crs' attribute of veg cover NetCFD from ARE NCI\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "veg_cover_data_path = os.path.join(working_dir, \"Data\", \"Vegetation_cover\", \"veg_cover_phd.nc\")\n",
    "veg_ds = xr.open_dataset(veg_cover_data_path)\n",
    "print(\n",
    "    \"The actual crs of the veg_ds is EPSG:32754. The attribute was incorrectly saved when downloading the data.\"\n",
    ")\n",
    "veg_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "2G_-HSNtQlMy",
    "outputId": "127941d9-bf29-4ddb-db47-534bf7da7d06"
   },
   "outputs": [],
   "source": [
    "# Visualise veg_cover of the first time step\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "veg_ds[\"veg_cover\"].isel(time=0).plot()\n",
    "plt.title(\"veg_cover layer (e.g. time=0)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 414
    },
    "id": "_YPA9EF5AdQd",
    "outputId": "f6e950b6-aeda-4f81-ca8d-a663b2638f90"
   },
   "outputs": [],
   "source": [
    "# Find coordinate of each observations in EPSG:32754\n",
    "\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32754\", always_xy=True)\n",
    "df['dea_x'], df['dea_y'] = transformer.transform(df['X'].values, df['Y'].values)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNSUuwdkIBFQ"
   },
   "outputs": [],
   "source": [
    "# Fill in veg cover in df row by row\n",
    "\n",
    "\n",
    "def get_veg_cover(row):\n",
    "    x = row['dea_x']\n",
    "    y = row['dea_y']\n",
    "    t = row['Datetime']\n",
    "\n",
    "    try:\n",
    "        # Select data at the given location across all times\n",
    "        point_series = veg_ds.sel(x=x, y=y, method='nearest')['veg_cover']\n",
    "\n",
    "        # Drop NaNs\n",
    "        valid_series = point_series.dropna(dim='time')\n",
    "\n",
    "        if valid_series.sizes['time'] == 0:\n",
    "            return np.nan\n",
    "\n",
    "        # Find the nearest time, within 30 days, with non-NaN veg_cover\n",
    "        time_deltas = np.abs(valid_series['time'] - np.datetime64(t))\n",
    "        if time_deltas.min() > np.timedelta64(30, 'D'):\n",
    "            print(\"min time_deltas: \", time_deltas.min() / np.timedelta64(1, 'D'))\n",
    "            return np.nan\n",
    "        nearest_idx = time_deltas.argmin().item()\n",
    "        veg_cover = valid_series.isel(time=nearest_idx).item()\n",
    "\n",
    "        return veg_cover if veg_cover < 100 else 100\n",
    "\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "df['veg_cover'] = df.apply(get_veg_cover, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKdd-fKERS-_"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['dea_x', 'dea_y'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQSVyLZFSHYB"
   },
   "source": [
    "# Save df to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw6bn10MNMzU"
   },
   "outputs": [],
   "source": [
    "# save df\n",
    "df = add_AEST_Datetime(df)\n",
    "df = df[~df['veg_cover'].isna()]\n",
    "df.to_csv(os.path.join(output_dir, 'in-situ_topography_phd.csv'), index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zioYV7qrLR52"
   },
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G98kJmH5OkCP"
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QfS4Kh2FUeW"
   },
   "source": [
    "## Investigation of correctness of codes above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSxCldxP-2By"
   },
   "outputs": [],
   "source": [
    "# Check for accuracy of the veg_cover section\n",
    "\n",
    "# Check for NaN and deal with it\n",
    "df.isna().sum()  # 9661 rows\n",
    "\n",
    "# Check for sites with some rows with NaN veg_cover\n",
    "df[df[\"veg_cover\"].isna()][\"SiteID\"].unique()  # Not all sites\n",
    "\n",
    "# Check for sites with all rows with NaN veg_cover\n",
    "nan_sites = df.groupby(\"SiteID\")[\"veg_cover\"].apply(lambda x: x.isna().all())\n",
    "nan_siteIDs = nan_sites[nan_sites].index.tolist()\n",
    "nan_siteIDs  # 0 site\n",
    "\n",
    "# Inspect different dates\n",
    "df[df['Datetime'].dt.strftime('%Y-%m-%d') == '2018-12-21']  # 120 rows\n",
    "df[df['Datetime'].dt.strftime('%Y-%m-%d') == '2019-01-20'].isna().sum()  # 48 rows\n",
    "df[df['Datetime'].dt.strftime('%Y-%m-%d') == '2019-01-20']  # 120 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PpnD_Lgz6R9y"
   },
   "outputs": [],
   "source": [
    "# Investigate whether I need to redownload the veg_cover data from NCI because of its insufficient coverage\n",
    "\n",
    "# Check whether the veg_cover of SiteID 67, 68, 69, 70 on 2018-12-21 are NaN or not\n",
    "df = pd.read_csv(os.path.join(output_dir, 'in-situ_topography_with_NaN_veg_cover.csv'))\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "first_date_df = df[\n",
    "    df['SiteID'].isin([67, 68, 69, 70]) & (df['Datetime'].dt.strftime('%Y-%m-%d') == '2018-12-21')\n",
    "]\n",
    "first_date_df['SiteID'].unique()\n",
    "\n",
    "# Check the last date\n",
    "last_date_df = df[\n",
    "    df['SiteID'].isin([302, 303, 304, 305])\n",
    "    & (df['Datetime'].dt.strftime('%Y-%m-%d') == '2020-11-06')\n",
    "]\n",
    "last_date_df['SiteID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NEW2wR5lBA2Z"
   },
   "outputs": [],
   "source": [
    "# Investigate site 302\n",
    "\n",
    "df = pd.read_csv(os.path.join(output_dir, 'in-situ_topography.csv'))\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "df[df['SiteID'].isin([302]) & (df['Datetime'].dt.strftime('%Y-%m-%d') == '2020-11-06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXobdojy_TGB"
   },
   "outputs": [],
   "source": [
    "# Investigate change in vegetation cover.\n",
    "from pyproj import Transformer\n",
    "\n",
    "transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32754\", always_xy=True)\n",
    "dea_x, dea_y = transformer.transform(150.3096, -35.46728)\n",
    "point_series = veg_ds.sel(x=dea_x, y=dea_y, method='nearest')['veg_cover']\n",
    "print(point_series.values)\n",
    "\n",
    "# visualise the point_series\n",
    "point_series.plot()\n",
    "plt.title(\"Vegetation cover of site 302\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GerMk1ZUXbEq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dem_path = os.path.join(working_dir, \"output\", \"batemans_bay_dem_no_neg.tif\")\n",
    "with rasterio.open(dem_path) as src:\n",
    "    dem_data = src.read(1)\n",
    "\n",
    "    lon = sites_with_nans_df['X'].tolist()\n",
    "    lat = sites_with_nans_df['Y'].tolist()\n",
    "    x_proj, y_proj = transform('EPSG:4326', src.crs, lon, lat)\n",
    "    rows, cols = rowcol(src.transform, x_proj, y_proj)\n",
    "\n",
    "plt.imshow(dem_data, cmap='terrain')\n",
    "plt.scatter(cols, rows, color='red', s=10)\n",
    "plt.colorbar(label='DEM (metres)')\n",
    "plt.title('DEM Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sleutg7NFg5m"
   },
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bStvCQ4PJtY8"
   },
   "outputs": [],
   "source": [
    "url = \"https://data.dea.ga.gov.au/derivative/ga_ls_fc_3/2-5-1/090/084/2015/01/02/ga_ls_fc_3_090084_2015-01-02_final_veg_cover.tif\"\n",
    "download_file(url, os.path.join(working_dir, \"data\", \"test.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-jgsDa67A1z"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(mosaic[0], cmap='terrain')\n",
    "plt.colorbar(label='Slope (degrees)')\n",
    "plt.title('Slope Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GShdEyN3SHDs"
   },
   "outputs": [],
   "source": [
    "# TODO (HIGH): Update df with the new code. Below is not usable.\n",
    "# add_data_to_df('slope')\n",
    "# orig_len = len(df)\n",
    "# df = df[(df['slope'] != np.nan) & (df['slope'] != -9999.0)]\n",
    "# print(\"Remove rows with no slope data.\")\n",
    "# print(f\"There are {orig_len - len(df)} rows out of {orig_len} rows with no slope data.\")\n",
    "# add_data_to_df('aspect')\n",
    "# add_data_to_df('relief')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ih_dsR9kUPmr"
   },
   "outputs": [],
   "source": [
    "# Visualise relief calculated\n",
    "# TODO (LOW): Correct scaling of the plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "relief_path = os.path.join(working_dir, \"output\", \"batemans_bay_relief.tif\")\n",
    "with rasterio.open(relief_path) as src:\n",
    "    relief_data = src.read(1)\n",
    "\n",
    "    lon = sites_with_nans_df['X'].tolist()\n",
    "    lat = sites_with_nans_df['Y'].tolist()\n",
    "    x_proj, y_proj = transform('EPSG:4326', src.crs, lon, lat)\n",
    "    rows, cols = rowcol(src.transform, x_proj, y_proj)\n",
    "\n",
    "plt.imshow(relief_data, cmap='terrain')\n",
    "plt.scatter(cols, rows, color='red', s=10)\n",
    "plt.colorbar(label='Relief (metres)')\n",
    "plt.title('Relief Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S3bpnpy2hViH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN8YVYex8KvTne1GXcKYa0k",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
