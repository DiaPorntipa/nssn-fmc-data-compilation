{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs_folder_path = (\n",
    "    'Data/pcs'  # Some observations were recorded twice. One with and one without gps data\n",
    ")\n",
    "output_file_path = 'Data/pcs/pcs.csv'\n",
    "\n",
    "plot_latlon_graph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.distance import great_circle\n",
    "from geopy.point import Point\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "sys.path.append('../..')\n",
    "from Utils.vpd import calculate_vpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Extract latlon of each site location using DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_latlon_graph(df, station_name):\n",
    "    df = df.dropna(subset=['Stn_lat', 'Stn_long'])\n",
    "    df = df[(df['Stn_lat'] != 999) & (df['Stn_long'] != 999)]\n",
    "    plt.scatter(df['Stn_long'], df['Stn_lat'], s=1)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.title(f'GPS Locations of station {station_name}')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'latlon_plots/station_{station_name}')\n",
    "\n",
    "\n",
    "def find_station_location(df, station_name):\n",
    "    # Prepare coordinates\n",
    "    latlon_df = df[['Stn_lat', 'Stn_long']]\n",
    "    latlon_df = latlon_df.drop_duplicates()\n",
    "    latlon_df = latlon_df.dropna()\n",
    "    coords = latlon_df.to_numpy()\n",
    "\n",
    "    # Run DBSCAN\n",
    "    kms_per_radian = 6371.0088\n",
    "    epsilon = 0.05 / kms_per_radian  # 50 meters radius\n",
    "    db = DBSCAN(eps=epsilon, min_samples=10, algorithm='ball_tree', metric='haversine').fit(\n",
    "        np.radians(coords)\n",
    "    )\n",
    "    latlon_df['cluster'] = db.labels_\n",
    "\n",
    "    # Get cluster centers\n",
    "    locations = latlon_df.groupby('cluster')[['Stn_lat', 'Stn_long']].mean()\n",
    "    locations = locations.loc[locations.index != -1]  # exclude noise\n",
    "    print(f\"locations fo station {station_name} are\", locations)\n",
    "\n",
    "    return locations, latlon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "glob.glob(os.path.join('..', '..', pcs_folder_path, 'fts*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dates_df_list = []\n",
    "df_list = []\n",
    "for i, file_name in enumerate(glob.glob(os.path.join('..', '..', pcs_folder_path, 'fts*.csv'))):\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(f\"Station names: {df['Station name'].unique()}\")\n",
    "    station_name = df['Station name'][0]\n",
    "\n",
    "    if plot_latlon_graph:\n",
    "        save_latlon_graph(df, station_name)\n",
    "    locations, latlon_df = find_station_location(df, station_name)\n",
    "    locations_df = locations.reset_index()\n",
    "\n",
    "    # Find the start and end dates of each cluster\n",
    "    df = df.merge(latlon_df, how='left', on=['Stn_lat', 'Stn_long'])\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    start_end_dates_df = df.groupby('cluster')['Date'].agg(['min', 'max']).reset_index()\n",
    "    start_end_dates_df.rename(\n",
    "        columns={'min': 'start_date_UTC', 'max': 'end_date_UTC'}, inplace=True\n",
    "    )\n",
    "    locations_dates_df = locations_df.merge(start_end_dates_df, on='cluster', how='left')\n",
    "\n",
    "    # Save locations_dates_df together in a list\n",
    "    locations_dates_df['Station_name'] = station_name\n",
    "    locations_dates_df_list.append(locations_dates_df)\n",
    "\n",
    "    # Save important site data together in a list\n",
    "    df = df[['Date', 'T', 'H', 'MF', 'MS', 'Station name', 'cluster']]\n",
    "    df_list.append(df[(~df['cluster'].isna()) & (df['cluster'] != -1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise sites_info_df\n",
    "\n",
    "sites_info_df = pd.concat(locations_dates_df_list, ignore_index=True)\n",
    "sites_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and save sites_info_df\n",
    "\n",
    "sites_info_df['start_date_UTC'] = sites_info_df['start_date_UTC'].dt.date\n",
    "sites_info_df['end_date_UTC'] = sites_info_df['end_date_UTC'].dt.date\n",
    "sites_info_df.drop(['cluster'], axis=1).to_csv(\"sites_latlon_dates.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "# Prepare pcs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign SiteID to each cluster\n",
    "\n",
    "sites_info_df['SiteID'] = [f'pcs_{i+1}' for i in range(len(sites_info_df))]\n",
    "sites_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all site data together and get the SiteID\n",
    "\n",
    "df_all = pd.concat(df_list, ignore_index=True)\n",
    "df_all = df_all.merge(\n",
    "    sites_info_df[['Station_name', 'cluster', 'SiteID', 'Stn_lat', 'Stn_long']],\n",
    "    left_on=['Station name', 'cluster'],\n",
    "    right_on=['Station_name', 'cluster'],\n",
    "    how='left',\n",
    ")\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_all.rename(\n",
    "    columns={\n",
    "        'Stn_long': 'X',\n",
    "        'Stn_lat': 'Y',\n",
    "        'Date': 'Datetime',\n",
    "        'T': 'Temperature',\n",
    "        'H': 'RH',\n",
    "        'MF': 'DFMC',\n",
    "        'MS': 'Soil_mois',\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n",
    "df_all['VPD'] = df_all.apply(lambda row: calculate_vpd(row['Temperature'], row['RH']), axis=1)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df_all\n",
    "\n",
    "df_all = df_all[['SiteID', 'X', 'Y', 'Datetime', 'Temperature', 'RH', 'VPD', 'DFMC', 'Soil_mois']]\n",
    "df_all.to_csv(os.path.join('..', '..', output_file_path), index=False)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check NaNs\n",
    "df_all[df_all.isna().any(axis=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
